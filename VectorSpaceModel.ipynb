{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'test/14826',\n",
       " u'test/14828',\n",
       " u'test/14829',\n",
       " u'test/14832',\n",
       " u'test/14833',\n",
       " u'test/14839',\n",
       " u'test/14840',\n",
       " u'test/14841',\n",
       " u'test/14842',\n",
       " u'test/14843']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters   \n",
    "reuters.fileids()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Normalization\n",
    "Inbuilt tokenizer 'word_tokenize' was used.\n",
    "\n",
    "All same tokens eleminated.\n",
    "\n",
    "All tokens converted to lowercase characters.\n",
    "\n",
    "Porter's stemmer and Snowball stemmer were explored.(Commented out because of poor performance.)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens=word_tokenize(reuters.raw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1548307"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63353"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueTokens=sorted(list(set(tokens))) #removing repeated tokens\n",
    "len(uniqueTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(uniqueTokens)):\n",
    "    uniqueTokens[i]=uniqueTokens[i].lower()\n",
    "uniqueTokens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52711"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.stem.porter import *\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "#stemmer = PorterStemmer()\n",
    "#stemmer=SnowballStemmer(\"english\")\n",
    "#for i in xrange(len(uniqueTokens)):\n",
    "    #uniqueTokens[i]=stemmer.stem(uniqueTokens[i])\n",
    "    \n",
    "uniqueTokens=list(set(uniqueTokens))\n",
    "len(uniqueTokens)                   #length after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'mid-week',\n",
       " u'1,800',\n",
       " u'fawc',\n",
       " u'degussa',\n",
       " u'woods',\n",
       " u'hanging',\n",
       " u'blgr',\n",
       " u'localized',\n",
       " u'161.9',\n",
       " u'161.8']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueTokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "Data structure used is dictionary (hash table) with keys as terms and values as posting lists implemented as variable length arrays.\n",
    "\n",
    "For handelling wild card queries trie data structure is implemented (refer to the C++ code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initializing the inverted index with empty posting list for every term in thr corpus.\n",
    "invIndex={}                \n",
    "for tok in uniqueTokens:\n",
    "    invIndex[tok]=[]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##inverted Index creation. \n",
    "#data structure used: dictionary(hash table) with posting list as a variable sized array\n",
    "\n",
    "for f in reuters.fileids():          #loop through each file\n",
    "    txt = reuters.raw(f)              \n",
    "    toks=[]                          #new list of tokens for each f\n",
    "    toks=word_tokenize(txt)          #tokenize and normalize file in a way identical  \n",
    "    for j in xrange(len(toks)):      #...as earlier to genenrate same terms\n",
    "        toks[j]=toks[j].lower() \n",
    "    #from nltk.stem.snowball import SnowballStemmer    \n",
    "    #stemmer = PorterStemmer()\n",
    "    #stemmer=SnowballStemmer(\"english\")\n",
    "    #for i in xrange(len(toks)):\n",
    "        #toks[i]=stemmer.stem(toks[i])   \n",
    "    toks=list(set(toks))             #remove identical terms within the same document\n",
    "    for tok in toks:\n",
    "        if tok not in uniqueTokens:\n",
    "            invIndex[tok]=[]\n",
    "        \n",
    "    for i in xrange(len(toks)):       #loop through each term of this doc\n",
    "        curPosList=invIndex[toks[i]]      \n",
    "        curPosList.append(f)        \n",
    "        invIndex[toks[i]]=curPosList  #locate that term in invIndex and append doc ID of f to ot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deleting entries in the inv index with empty posting list if any\n",
    "for key in invIndex.keys():\n",
    "    lis=[]\n",
    "    lis=invIndex[key]\n",
    "    if len(lis)==0:\n",
    "        del invIndex[key]\n",
    "len(invIndex.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#persisting inverted index to disk\n",
    "import pickle\n",
    "pickle.dump( invIndex, open( \"save.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading inverted index back from the disk\n",
    "import pickle\n",
    "invIndex = pickle.load( open( \"save.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'test/14849', u'test/14861', u'test/14959', u'test/15048', u'test/15095']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invIndex['main'][0:5]        #first five terms of the posting list of a random term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: TFIDF calculation and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a new index that stores the tf value of every term and doc.\n",
    "#structure is similar to that of inverted index, instead of posting list of docIDs\n",
    "#...the value of hashtable is a list of tuples (docID,tf(t,d))\n",
    "\n",
    "tfIndex={}\n",
    "for key in invIndex.keys():\n",
    "    tfIndex[key]=[]\n",
    "    curPosLis=invIndex[key]\n",
    "    for docID in curPosLis:\n",
    "        tf=0\n",
    "        txt = reuters.raw(docID)              \n",
    "        toks=[]                    \n",
    "        toks=word_tokenize(txt)            \n",
    "        for j in xrange(len(toks)):      \n",
    "            toks[j]=toks[j].lower()\n",
    "        tf=toks.count(key)\n",
    "        tfIndex[key].append([docID,tf])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'test/16926', 2],\n",
       " [u'test/17038', 1],\n",
       " [u'test/17480', 1],\n",
       " [u'test/17486', 1],\n",
       " [u'test/17509', 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing how posting list of a random word looks like in this index\n",
    "tfIndex['main'][15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#persisting tfindex index to disk\n",
    "import pickle\n",
    "pickle.dump( tfIndex, open( \"tfindex.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading tf index back from the disk\n",
    "import pickle\n",
    "tfIndex = pickle.load( open( \"tfindex.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788\n"
     ]
    }
   ],
   "source": [
    "N=len(reuters.fileids())\n",
    "print N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52708\n"
     ]
    }
   ],
   "source": [
    "V=len(invIndex.keys())\n",
    "print V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a dictionary of idfs:\n",
    "#    ---keys: term\n",
    "#    ---values: IDF==>log(N/(length of posting list of the corresponding terms))\n",
    "\n",
    "import math\n",
    "idf={}\n",
    "for i in xrange(V):\n",
    "    lis=invIndex[invIndex.keys()[i]]\n",
    "    df=len(lis)\n",
    "    if df!=0:\n",
    "        idf[invIndex.keys()[i]]=math.log(N/df)\n",
    "    else:\n",
    "        idf[invIndex.keys()[i]]=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7376696182833684"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\xfc' in position 7: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c15b1f2ae95e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m        \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xfc' in position 7: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "#writing the dictionary into a csv file \n",
    "import csv\n",
    "with open('dict.csv', 'wb') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in idf.items():\n",
    "       writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52708"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "U=np.zeros((V,N))\n",
    "docID=[]\n",
    "docID=reuters.fileids()\n",
    "terms=[]\n",
    "terms=invIndex.keys()\n",
    "\n",
    "#creating tfidf matrix\n",
    "\n",
    "for i in xrange(V):                         #for i th word in the inverted index\n",
    "    posLis=tfIndex[invIndex.keys()[i]]      #access posting list of tfIndex\n",
    "    for doc in posLis:                      #doc is a tuple of format (docID,tf)\n",
    "        fileid=doc[0]\n",
    "        tf=1+math.log(doc[1])\n",
    "        r=i\n",
    "        c=reuters.fileids().index(fileid)\n",
    "        U[r,c]=tf*idf[terms[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7376696182833684"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reuters.fileids().index('test/14849')            \n",
    "#11\n",
    "#invIndex.keys().index('main')\n",
    "#46810\n",
    "U[46810,11]             #idf*tf=3.73*1 as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3284247760610528"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids().index('test/16926') #'test/16926', 2]\n",
    "#1190\n",
    "U[46810,1190]                       #idf*tf=3.73*2 as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converts a vector represented by a list of numpy array into a unit vector\n",
    "def unitNorm(lis):\n",
    "    norm=0.0\n",
    "    for i in xrange(len(lis)):\n",
    "        norm+=lis[i]**2.0\n",
    "    norm=norm**(0.5)\n",
    "    for i in xrange(len(lis)):\n",
    "        lis[i]=lis[i]/norm\n",
    "        \n",
    "    return lis\n",
    "\n",
    "#computes cosine similarity between two unit normalised vectors\n",
    "def cosineSim(vec1,vec2):\n",
    "    #vec1=unitNorm(vec1)\n",
    "    #vec2=unitNorm(vec2)\n",
    "    if len(vec1)!=len(vec2):\n",
    "        print 'vector length not same'\n",
    "        return\n",
    "    else:\n",
    "        sim=0\n",
    "        for i in xrange(len(vec2)):\n",
    "            sim+=vec1[i]*vec2[i]\n",
    "        return sim   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unit normalize vector of each document\n",
    "for i in xrange(N):\n",
    "    vec=U[:,i]\n",
    "    U[:,i]=unitNorm(vec)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.062063432186458295"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[46810,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saving the U matrix to disk\n",
    "np.save('/tmp/123', U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the matrix back from the disk\n",
    "import numpy as np\n",
    "np.load('/tmp/123.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSim([1,1,1],[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V=len(invIndex.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52708,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(qVector)\n",
    "qVector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52708, 10788)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52708,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52708, 10788)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reuters.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Online Computation\n",
    "\n",
    "Inverted index, tfidf matrix will be computed offline and saved before hand. When the user gives a query only the following code will run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computes cosine similarity between the query vector and document vectors and returns the docID\n",
    "#of the top three similar docs in decreasing order of similarity\n",
    "#(efficient implementation)\n",
    "def cosineScore(q,queryVector):\n",
    "    scores=[0 for x in xrange(N)]  #scores is a list of length N and accumulates the score of each\n",
    "                                   #document (initialize to zero)\n",
    "    \n",
    "    for term in q:                  \n",
    "        if term in invIndex.keys(): #walk through the posting list of each term in the query that is present in the corpus \n",
    "            posLis=invIndex[term]\n",
    "            for doc in posLis:\n",
    "                docIndex=reuters.fileids().index(doc)\n",
    "                termIndex=invIndex.keys().index(term)\n",
    "                scores[docIndex]+=U[termIndex,docIndex]*queryVector[termIndex]\n",
    "                \n",
    "    SCORES=scores\n",
    "    #sort the scores list and return the last three docs\n",
    "    maxLis=sorted(SCORES)               \n",
    "    maxSim=maxLis[-1]\n",
    "    Maxindex=scores.index(maxSim)\n",
    "    doc=reuters.fileids()[Maxindex]     #most similar doc\n",
    "    \n",
    "    maxSim2=maxLis[-2]\n",
    "    Maxindex=scores.index(maxSim2)\n",
    "    doc1=reuters.fileids()[Maxindex]    #second most similar doc\n",
    "    \n",
    "    maxSim3=maxLis[-3]\n",
    "    Maxindex=scores.index(maxSim3)\n",
    "    doc2=reuters.fileids()[Maxindex]    #third most similar doc\n",
    "    return [doc, doc1, doc2]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillsbury company\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'training/7106', u'training/1065', u'training/10073']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing of query and conversion into vector\n",
    "\n",
    "query=raw_input()  #take input query from the user\n",
    "\n",
    "toks=word_tokenize(query)\n",
    "for i in xrange(len(toks)):\n",
    "    toks[i]=toks[i].lower()         #tokenize and normalize the query in the same way as the corpus\n",
    "#stemmer = PorterStemmer()\n",
    "#stemmer=SnowballStemmer(\"english\")\n",
    "#for i in xrange(len(toks)):\n",
    "    #toks[i]=stemmer.stem(toks[i])\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "qVector=np.zeros(V)             #store the query vector in a numpy array\n",
    "for term in toks:\n",
    "    if term in invIndex.keys():\n",
    "        qVector[invIndex.keys().index(term)]=idf[term]\n",
    "qVector=unitNorm(qVector)\n",
    "\n",
    "\n",
    "cosineScore(toks,qVector)\n",
    "#for example take the query \"Pillsbury Company\". The documents returned are [u'training/7106', u'training/1065', u'training/10073']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PILLSBURY CO 3RD QTR SHR 56 CTS VS 63 CTS\n",
      "\n",
      "  PILLSBURY CO 3RD QTR SHR 56 CTS VS 63 CTS\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print reuters.raw(cosineScore(toks,qVector)[0]) #even though it does not have the word company\n",
    "#this is the first result because the word Pillsbury is a relatively rare word and has a higher\n",
    "#idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PILLSBURY CO &lt;PSY> VOTES QUARTERLY DIVIDEND\n",
      "  Qtly div 25 cts vs 25 cts prior qtr\n",
      "      Pay 31 May\n",
      "      Record 1 May\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print reuters.raw(cosineScore(toks,qVector)[1]) #similar to the first result, lower because of\n",
    "#tf=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PILLSBURY &lt;PSY> FILES FOR SECOND BURGER KING MLP\n",
      "  The Pillsbury Co said it\n",
      "  filed a registration statement with the Securities and Exchange\n",
      "  Commission for the sale of limited partnership interests in a\n",
      "  second master limited partnership of its Burger King unit's\n",
      "  restaurant properties.\n",
      "      Pillsbury said it expects the offering to yield 73-82 mln\n",
      "  dlrs, resulting in an after-tax gain of 20-23 mln dlrs.\n",
      "      A spokesman for Pillsbury said the company is aiming to get\n",
      "  this after-tax gain in the fourth fiscal quarter ending in May.\n",
      "      Pillsbury said the sale will occur as soon as practicable,\n",
      "  considering the business and legal contigencies.\n",
      "      The company said Burger King and another Pillsbury unit,\n",
      "  QSV Properties, will be the master limited partnership's\n",
      "  general partner.\n",
      "      Pillsbury said it expects the interests to be sold to\n",
      "  public investors and listed for trading on the New York Stock\n",
      "  Exchange.\n",
      "      Merrill Lynch will lead the underwriting, Pillsbury said.\n",
      "      Pillsbury first sold limited partnership interests in\n",
      "  Burger King Investors L.P. in February 1986.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print reuters.raw(cosineScore(toks,qVector)[2]) #even though this has tf of pillsbury greater\n",
    "#than the first two results, it has lesser weight because the effect is diluted due to length \n",
    "#normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDITIONAL:\n",
    "\n",
    "The next additional component leverages the word2vec model to retrieve documents intuitively similar to the querry.\n",
    "About word2vec (from wikipedia):\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "\n",
    "\n",
    "Paper on word2vec: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import gensim #optimized library that uses deep learning to generate skip gram model\n",
    "              # to generate word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "#training the reuters corpus and forming a gensim model\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = reuters.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now learnt word vectors for all the words in the corpus. The vectors learnt capture the context of the word and the words which appear in similar context have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'very', 0.9034268856048584),\n",
       " (u'positive', 0.8626291751861572),\n",
       " (u'bad', 0.8152326345443726),\n",
       " (u'significant', 0.8105217218399048),\n",
       " (u'better', 0.8077592849731445)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'group', 0.7351366877555847),\n",
       " (u'transaction', 0.7068047523498535),\n",
       " (u'firm', 0.6975889205932617),\n",
       " (u'partnership', 0.6868374347686768),\n",
       " (u'acquisition', 0.6747386455535889)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('company',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'energy', 0.7945944666862488),\n",
       " (u'exporters', 0.7928650379180908),\n",
       " (u'field', 0.774497926235199),\n",
       " (u'shipping', 0.766197919845581),\n",
       " (u'sector', 0.7620868682861328)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('industry')[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'key', 0.8374419212341309),\n",
       " (u'biggest', 0.8255422115325928),\n",
       " (u'affecting', 0.7950398921966553),\n",
       " (u'weakness', 0.7907974720001221),\n",
       " (u'world', 0.7907313108444214)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('main')[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'chairman', 0.957267165184021),\n",
       " (u'director', 0.9501000642776489),\n",
       " (u'chief', 0.9411285519599915),\n",
       " (u'executive', 0.913731575012207),\n",
       " (u'vice', 0.9005922079086304)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('president')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a user now gives in a query we will also append the the top 2 similar words to the query. This will increase our recall. But since reuters is a relatively small corpus, the similar words might not be relevant all the time. So in order to not harm the precision too much, the weights of these similar words are halved so that the original words of the query are given more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newCosineScore(q,queryVector):\n",
    "    scores=[0 for x in xrange(N)]  #scores is a list of length N and accumulates the score of each\n",
    "                                   #document (initialize to zero)\n",
    "    \n",
    "    for term in q:                  \n",
    "        if term in invIndex.keys(): #walk through the posting list of each term in the query that is present in the corpus \n",
    "            posLis=invIndex[term]\n",
    "            for doc in posLis:\n",
    "                docIndex=reuters.fileids().index(doc)\n",
    "                termIndex=invIndex.keys().index(term)\n",
    "                scores[docIndex]+=U[termIndex,docIndex]*queryVector[termIndex]\n",
    "            similarTerms=model.most_similar(term)[0:3]\n",
    "            for simTerm in similarTerms: #also add weights to the words similar to the query vectors\n",
    "                posLis=invIndex[simTerm[0]]\n",
    "                for doc in posLis:\n",
    "                    docIndex=reuters.fileids().index(doc)\n",
    "                    termIndex=invIndex.keys().index(simTerm[0])\n",
    "                    scores[docIndex]+=(U[termIndex,docIndex]*queryVector[termIndex])\n",
    "                                          #Note that we have added the weight to be half of tfidf weight\n",
    "                \n",
    "    SCORES=scores\n",
    "    #sort the scores list and return the last three docs\n",
    "    maxLis=sorted(SCORES)               \n",
    "    maxSim=maxLis[-1]\n",
    "    Maxindex=scores.index(maxSim)\n",
    "    doc=reuters.fileids()[Maxindex]     #most similar doc\n",
    "    \n",
    "    maxSim2=maxLis[-2]\n",
    "    Maxindex=scores.index(maxSim2)\n",
    "    doc1=reuters.fileids()[Maxindex]    #second most similar doc\n",
    "    \n",
    "    maxSim3=maxLis[-3]\n",
    "    Maxindex=scores.index(maxSim3)\n",
    "    doc2=reuters.fileids()[Maxindex]    #third most similar doc\n",
    "    return [doc, doc1, doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad crops\n"
     ]
    }
   ],
   "source": [
    "#preprocessing of query and conversion into vector\n",
    "\n",
    "query=raw_input()\n",
    "\n",
    "toks=word_tokenize(query)\n",
    "for i in xrange(len(toks)):\n",
    "    toks[i]=toks[i].lower()         #tokenize and normalize the query in the same way as the corpus\n",
    "    \n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "#stemmer=SnowballStemmer(\"english\")\n",
    "#for i in xrange(len(toks)):\n",
    "    #toks[i]=stemmer.stem(toks[i])\n",
    "    \n",
    "qVector=np.zeros(V)             #Old query vector containing non zero dimensions \n",
    "                                #...only for words in the query\n",
    "for term in toks:\n",
    "    if term in invIndex.keys():\n",
    "        qVector[invIndex.keys().index(term)]=idf[term]\n",
    "qVector=unitNorm(qVector)\n",
    "    \n",
    "    \n",
    "#the new query vector qVector1 with appended dimensions of similar terms\n",
    "qVector1=np.zeros(V)             \n",
    "for term in toks:\n",
    "    if term in invIndex.keys():\n",
    "        qVector1[invIndex.keys().index(term)]=idf[term]\n",
    "        \n",
    "for term in toks:\n",
    "    if term in invIndex.keys():\n",
    "        lis=model.most_similar(term)[0:3]\n",
    "        for item in lis:\n",
    "            qVector1[invIndex.keys().index(item[0])]=idf[item[0]]/2 #halving weights of added terms                    \n",
    "qVector1=unitNorm(qVector1)\n",
    "\n",
    "\n",
    "lis=newCosineScore(toks,qVector1)\n",
    "\n",
    "#consider the query example 'bad crops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'training/1623', u'training/1299', u'test/21168']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis #the new result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'test/21168', u'training/6494', u'test/15574']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineScore(toks,qVector) #result from the earlier model without word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'IZVESTIA SAYS SOVIET WINTER CROPS NEED RESEEDING\\n  The government daily Izvestia said a\\n  considerable amount of Soviet winter crops need to be reseeded\\n  and the state 1987 grain harvest target of 232 mln tonnes will\\n  not be easy to fulfil.\\n      Without giving figures, the newspaper said: \"A considerable\\n  part of the winter crops must be reseeded, but that creates\\n  extra effort in the fields in spring.\"\\n      The Soviet Union has previously said nine mln hectares of\\n  winter grain will have to be reseeded because of winterkill.\\n      A U.S. Department of Agriculture analyst in Washington has\\n  said the figure of nine mln hectares would equal about 25 pct\\n  of the total winter crop and would be the second highest\\n  winterkill in 10 years.\\n      \"The planned task of bringing in no less than 232 mln tonnes\\n  of grain is not simple,\" Izvestia said.\\n      This week\\'s sudden fall in temperatures has affected large\\n  parts of the country and has caused fieldwork to stop in the\\n  Ukraine, it said, adding that temperatures fell to as low as\\n  minus 30 centigrade in Byelorussia.\\n  \\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.raw('training/1623') #an intuitive article about crops going bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'U.S. SENATE PANEL VOTES TO LIMIT COUNTY LOAN RATE  CHANGES STARTING WITH 1988 CROPS\\n\\n  U.S. SENATE PANEL VOTES TO LIMIT COUNTY LOAN RATE  CHANGES STARTING WITH 1988 CROPS\\n  \\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.raw('test/21168') #just a small article that has crops many times and completely\n",
    "                          #ignores bad because of its small idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archives\n",
    "Earlier implementations that we considered but did not use because of inefficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing tfidf values in a dictionary of dictionary:\n",
    "#termDoc is a dictionary which has all terms as keys and each key has a dictionary as its values\n",
    "#each new dictionary has the doc ids in which the term appears and value is tf score of \n",
    "#that term and document.\n",
    "\n",
    "termDoc={}\n",
    "lis=[]\n",
    "for i in xrange(len(invIndex.keys())):\n",
    "    dic={}\n",
    "    lis.append(dic)\n",
    "    \n",
    "for term in invIndex.keys():\n",
    "    dic={}\n",
    "    dic=lis[invIndex.keys().index(term)]                    #termDoc[term]=dic\n",
    "    for doc in invIndex[term]:\n",
    "        IDF=idf[term]\n",
    "        TF=getTF(term,doc)\n",
    "        if TF!=0:\n",
    "            dic[doc]=IDF*(1+math.log(TF))\n",
    "            \n",
    "    termDoc[term]=dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alternative general purpose methods for calculating tf and df for a single term and document\n",
    "#(not used in the current system design)\n",
    "\n",
    "def getDF(term):\n",
    "    posLis=[]\n",
    "    posLis=invIndex[term]\n",
    "    return len(posLis)\n",
    "\n",
    "def getTF(term,doc):\n",
    "    txt = reuters.raw(f)\n",
    "    toks=[]\n",
    "    toks=word_tokenize(txt)\n",
    "    for i in xrange(len(toks)):\n",
    "        toks[i]=toks[i].lower()\n",
    "    #stemmer = PorterStemmer()\n",
    "    #stemmer=SnowballStemmer(\"english\")\n",
    "    #for i in xrange(len(toks)):\n",
    "        #toks[i]=stemmer.stem(toks[i])\n",
    "    return toks.count(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive implementation of storing TFIDF values in a N*V matrix\n",
    "\n",
    "\n",
    "termDoc=[]\n",
    "for i in xrange(V):\n",
    "    lis=[]\n",
    "    for j in xrange(N):\n",
    "        tf=getTF(invIndex.keys()[i],reuters.fileids()[j])\n",
    "        if tf!=0:\n",
    "            lis.append(1+math.log(tf))\n",
    "    termDoc.append(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#naive cosine similarity checking with each document vector\n",
    "doc=0\n",
    "maxSim=0\n",
    "\n",
    "for i in xrange(N):\n",
    "    docVec=U[:,i]\n",
    "    simTemp=cosineSim(queryVector,docVec)\n",
    "    if simTemp>maxSim:\n",
    "        maxSim=simTemp\n",
    "        doc=i\n",
    "\n",
    "print reuters.fileids()[doc]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc=0\n",
    "maxSim=0\n",
    "\n",
    "for term in queryVector:\n",
    "    for doc in invIndex[term]:\n",
    "        fileIndex=reuters.fileids().index(doc)\n",
    "        docVec=U[:,fileIndex]\n",
    "        simTemp=cosineSim(queryVector,docVec)\n",
    "        if simTemp>maxSim:\n",
    "            maxSim=simTemp\n",
    "            DOC=doc\n",
    "\n",
    "print DOC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
